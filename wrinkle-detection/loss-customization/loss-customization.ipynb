{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract proportion of zeros/ones\n",
    "* from *image-processing/DATA/ORIGINAL/label-npy/*\n",
    "* from *image-processing/DATA/CROPPED/label-npy/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MEAN PROPORTION: 0.004195659563537643\n",
      "----------------------------------------------------------------------------------\n",
      "PROPORTION (= nnz/tot)\t<= (nnz, nz)\t\tlabel_file_name\n",
      "0.0010133658119362444\t<= (1699, 1674892) \t0001_f_22_18_eye_closed_tex.npy\n",
      "0.0028050967707687804\t<= (4703, 1671888) \t0004_f_20_15_lip_roll_tex.npy\n",
      "0.002675667470480278\t<= (4486, 1672105) \t0007_f_20_20_brow_lower_tex.npy\n",
      "0.0018889520461460188\t<= (3167, 1673424) \t0002_m_24_14_sadness_tex.npy\n",
      "0.001932492778501137\t<= (3240, 1673351) \t0004_f_20_13_lip_funneler_tex.npy\n",
      "0.005873227280833548\t<= (9847, 1666744) \t0005_f_45_15_lip_roll_tex.npy\n",
      "0.004891473233483897\t<= (8201, 1668390) \t0011_f_45_07_jaw_forward_tex.npy\n",
      "0.0025581671379603017\t<= (4289, 1672302) \t0010_f_22_14_sadness_tex.npy\n",
      "0.006073037490956351\t<= (10182, 1666409) \t0013_m_20_19_brow_raiser_tex.npy\n",
      "0.003155211974775005\t<= (5290, 1671301) \t0013_m_20_11_chin_raiser_tex.npy\n",
      "0.0033502505977903974\t<= (5617, 1670974) \t0011_f_45_18_eye_closed_tex.npy\n",
      "0.002714436615727986\t<= (4551, 1672040) \t0016_f_22_06_jaw_right_tex.npy\n",
      "0.002014206207715537\t<= (3377, 1673214) \t0016_f_22_20_brow_lower_tex.npy\n",
      "0.0030699198552300473\t<= (5147, 1671444) \t0006_m_24_17_cheek_blowing_tex.npy\n",
      "0.004184681893198759\t<= (7016, 1669575) \t0011_f_45_01_neutral_tex.npy\n",
      "0.004710152923402309\t<= (7897, 1668694) \t0005_f_45_03_mouth_stretch_tex.npy\n",
      "0.009408973327424517\t<= (15775, 1660816) \t0014_f_58_19_brow_raiser_tex.npy\n",
      "0.004080899873612587\t<= (6842, 1669749) \t0009_f_23_06_jaw_right_tex.npy\n",
      "0.0022313134211027017\t<= (3741, 1672850) \t0006_m_24_05_jaw_left_tex.npy\n",
      "0.0017851700265598468\t<= (2993, 1673598) \t0004_f_20_03_mouth_stretch_tex.npy\n",
      "0.0034784870013020467\t<= (5832, 1670759) \t0001_f_22_02_smile_tex.npy\n",
      "0.002491961366844985\t<= (4178, 1672413) \t0010_f_22_06_jaw_right_tex.npy\n",
      "0.006888978886323498\t<= (11550, 1665041) \t0014_f_58_05_jaw_left_tex.npy\n",
      "0.0029756810098586954\t<= (4989, 1671602) \t0010_f_22_07_jaw_forward_tex.npy\n",
      "0.0036264062016317634\t<= (6080, 1670511) \t0009_f_23_13_lip_funneler_tex.npy\n",
      "0.004008729618613007\t<= (6721, 1669870) \t0004_f_20_17_cheek_blowing_tex.npy\n",
      "0.00321366391684078\t<= (5388, 1671203) \t0005_f_45_17_cheek_blowing_tex.npy\n",
      "0.002507469024944068\t<= (4204, 1672387) \t0010_f_22_12_lip_puckerer_tex.npy\n",
      "0.0031158463811388703\t<= (5224, 1671367) \t0007_f_20_10_dimpler_tex.npy\n",
      "0.0034796798980788993\t<= (5834, 1670757) \t0002_m_24_08_mouth_left_tex.npy\n",
      "0.003995607754067629\t<= (6699, 1669892) \t0009_f_23_20_brow_lower_tex.npy\n",
      "0.001998698549616454\t<= (3351, 1673240) \t0005_f_45_13_lip_funneler_tex.npy\n",
      "0.008390239479992437\t<= (14067, 1662524) \t0015_m_50_19_brow_raiser_tex.npy\n",
      "0.003681875901755407\t<= (6173, 1670418) \t0009_f_23_19_brow_raiser_tex.npy\n",
      "0.00765601151383969\t<= (12836, 1663755) \t0014_f_58_04_anger_tex.npy\n",
      "0.008731407958172267\t<= (14639, 1661952) \t0014_f_58_12_lip_puckerer_tex.npy\n",
      "0.0033633724623357755\t<= (5639, 1670952) \t0010_f_22_05_jaw_left_tex.npy\n",
      "0.003016239500271682\t<= (5057, 1671534) \t0006_m_24_06_jaw_right_tex.npy\n",
      "0.001779205542675584\t<= (2983, 1673608) \t0009_f_23_15_lip_roll_tex.npy\n",
      "0.003084234616552278\t<= (5171, 1671420) \t0003_m_30_12_lip_puckerer_tex.npy\n",
      "0.002544448825026497\t<= (4266, 1672325) \t0008_f_25_12_lip_puckerer_tex.npy\n",
      "0.0023166055406476592\t<= (3884, 1672707) \t0002_m_24_12_lip_puckerer_tex.npy\n",
      "0.0025897789025468945\t<= (4342, 1672249) \t0004_f_20_01_neutral_tex.npy\n",
      "0.0026875964382488036\t<= (4506, 1672085) \t0006_m_24_15_lip_roll_tex.npy\n",
      "0.001436844167718901\t<= (2409, 1674182) \t0002_m_24_07_jaw_forward_tex.npy\n",
      "0.001988558927013207\t<= (3334, 1673257) \t0007_f_20_12_lip_puckerer_tex.npy\n",
      "0.002182404653251747\t<= (3659, 1672932) \t0007_f_20_11_chin_raiser_tex.npy\n",
      "0.006604473005044164\t<= (11073, 1665518) \t0003_m_30_16_grin_tex.npy\n",
      "0.004909366685136685\t<= (8231, 1668360) \t0008_f_25_04_anger_tex.npy\n",
      "0.005538023286537981\t<= (9285, 1667306) \t0009_f_23_17_cheek_blowing_tex.npy\n",
      "0.0025486239637454813\t<= (4273, 1672318) \t0001_f_22_11_chin_raiser_tex.npy\n",
      "0.006385576446491721\t<= (10706, 1665885) \t0014_f_58_18_eye_closed_tex.npy\n",
      "0.007660186652558674\t<= (12843, 1663748) \t0017_m_31_07_jaw_forward_tex.npy\n",
      "0.0025796392799436476\t<= (4325, 1672266) \t0010_f_22_04_anger_tex.npy\n",
      "0.010013175544900336\t<= (16788, 1659803) \t0015_m_50_07_jaw_forward_tex.npy\n",
      "0.009358871662796711\t<= (15691, 1660900) \t0020_m_43_02_smile_tex.npy\n",
      "0.0030102750163874194\t<= (5047, 1671544) \t0008_f_25_02_smile_tex.npy\n",
      "0.0045151143003869165\t<= (7570, 1669021) \t0013_m_20_08_mouth_left_tex.npy\n",
      "0.008641344251519899\t<= (14488, 1662103) \t0015_m_50_18_eye_closed_tex.npy\n",
      "0.002265907427631426\t<= (3799, 1672792) \t0001_f_22_03_mouth_stretch_tex.npy\n",
      "0.008554262786809663\t<= (14342, 1662249) \t0014_f_58_02_smile_tex.npy\n",
      "0.0030096785679989933\t<= (5046, 1671545) \t0013_m_20_03_mouth_stretch_tex.npy\n",
      "0.003560796878904873\t<= (5970, 1670621) \t0010_f_22_08_mouth_left_tex.npy\n",
      "0.0035518501530784788\t<= (5955, 1670636) \t0010_f_22_16_grin_tex.npy\n",
      "0.005474203308976369\t<= (9178, 1667413) \t0011_f_45_08_mouth_left_tex.npy\n",
      "0.004982729836913117\t<= (8354, 1668237) \t0009_f_23_07_jaw_forward_tex.npy\n",
      "0.0037987797858869575\t<= (6369, 1670222) \t0016_f_22_10_dimpler_tex.npy\n",
      "0.005504025728397683\t<= (9228, 1667363) \t0009_f_23_14_sadness_tex.npy\n",
      "0.0005702046593355207\t<= (956, 1675635) \t0007_f_20_13_lip_funneler_tex.npy\n",
      "0.0063784190658306055\t<= (10694, 1665897) \t0005_f_45_11_chin_raiser_tex.npy\n",
      "0.0029333331742804297\t<= (4918, 1671673) \t0016_f_22_11_chin_raiser_tex.npy\n",
      "0.003763589330969807\t<= (6310, 1670281) \t0008_f_25_10_dimpler_tex.npy\n",
      "0.0047644297267491\t<= (7988, 1668603) \t0008_f_25_19_brow_raiser_tex.npy\n",
      "0.011410654118983104\t<= (19131, 1657460) \t0014_f_58_20_brow_lower_tex.npy\n",
      "0.004886701646376486\t<= (8193, 1668398) \t0004_f_20_05_jaw_left_tex.npy\n",
      "0.0023040801244907077\t<= (3863, 1672728) \t0008_f_25_18_eye_closed_tex.npy\n",
      "0.0025772534863899425\t<= (4321, 1672270) \t0013_m_20_01_neutral_tex.npy\n",
      "0.006045004416700316\t<= (10135, 1666456) \t0005_f_45_08_mouth_left_tex.npy\n",
      "0.0057098004224047485\t<= (9573, 1667018) \t0011_f_45_06_jaw_right_tex.npy\n",
      "Total mean 0.004195659563537642\n"
     ]
    }
   ],
   "source": [
    "# original\n",
    "label_path = \"../../image-processing/DATA/ORIGINAL/label-npy/\"\n",
    "# cropped\n",
    "label_path = \"../../image-processing/DATA/CROPPED/label-npy/\"\n",
    "\n",
    "label_list = [(label_file_name, np.load(label_path + label_file_name)) for label_file_name in os.listdir(label_path) if label_file_name.endswith(\".npy\")]\n",
    "proportion_list = [np.mean(label) for _, label in label_list]\n",
    "\n",
    "print('MEAN PROPORTION:', np.mean(proportion_list))\n",
    "print('----------------------------------------------------------------------------------')\n",
    "print('PROPORTION (= nnz/tot)\\t<= (nnz, nz)\\t\\tlabel_file_name')\n",
    "total_mean = 0\n",
    "i = 0\n",
    "for label_file_name, label in label_list:\n",
    "    total_mean += np.mean(label)\n",
    "    i += 1\n",
    "    print(np.mean(label), end='\\t<= ')\n",
    "    print((label.sum(), (label == 0).sum()), '\\t' + label_file_name)\n",
    "print(\"Total mean\", total_mean / i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customize loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6192, 0.5774]])\n",
      "tensor([[-0.5269, -0.0957]])\n",
      "tensor([[0.0243, 0.0227],\n",
      "        [0.1819, 0.1697]])\n"
     ]
    }
   ],
   "source": [
    "def my_loss(output, target):\n",
    "    loss = torch.mean((output - target)**2)\n",
    "    return loss\n",
    "\n",
    "model = nn.Linear(2, 2)\n",
    "x = torch.randn(1, 2)\n",
    "print(x)\n",
    "target = torch.randn(1, 2)\n",
    "print(target)\n",
    "output = model(x)\n",
    "loss = my_loss(output, target)\n",
    "loss.backward()\n",
    "print(model.weight.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimal F-1 loss\n",
    "* https://www.kaggle.com/rejpalcz/best-loss-function-for-f1-score-metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output(-inf ~ +inf) target(0 or 1)\n",
    "def f1_loss(output, target):\n",
    "    eps = torch.finfo(torch.float64).eps\n",
    "    y_pred = torch.sigmoid(output)\n",
    "    y_true = target\n",
    "    \n",
    "    tp = torch.sum(y_true*y_pred)\n",
    "    tn = torch.sum((1-y_true)*(1-y_pred))\n",
    "    fp = torch.sum((1-y_true)*y_pred)\n",
    "    fn = torch.sum(y_true*(1-y_pred))\n",
    "    \n",
    "    p = tp / (tp + fp + eps)\n",
    "    r = tp / (tp + fn + eps)\n",
    "    f1 = 2*p*r / (p+r+eps)\n",
    "    f1 = torch.where(torch.isnan(f1), torch.zeros_like(f1), f1)\n",
    "    return 1 - torch.mean(f1)\n",
    "\n",
    "def customized_loss(output, target):\n",
    "    return f1_loss(output, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 2, 3]) torch.Size([1, 1, 2, 3])\n",
      "tensor(0.2327)\n"
     ]
    }
   ],
   "source": [
    "output = torch.ones(1, 1, 2, 3) * 0.5\n",
    "target = torch.ones(1, 1, 2, 3)\n",
    "print(output.shape, target.shape)\n",
    "print(customized_loss(output, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----target-----\n",
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "torch.Size([5, 30]) torch.Size([5, 30])\n",
      "tensor(0.4612, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "target = torch.tensor(np.load('0001_f_22_02_smile_tex.npy'), dtype=torch.float64)\n",
    "\n",
    "target = target[399:404, 300:330]\n",
    "print('-----target-----')\n",
    "print(np.asarray(target, dtype='int'))\n",
    "\n",
    "output = torch.ones(target.shape)\n",
    "print(output.shape, target.shape)\n",
    "print(customized_loss(output, target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weighted BCE loss\n",
    "* https://discuss.pytorch.org/t/solved-class-weight-for-bceloss/3114"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output(0 ~ 1) target(0 or 1)\n",
    "def weighted_binary_cross_entropy(output, target, weights=None):\n",
    "    eps = torch.finfo(torch.float64).eps\n",
    "    if weights is not None:\n",
    "        assert len(weights) == 2\n",
    "        loss = weights[1] * (target * torch.log(output + eps)) + \\\n",
    "               weights[0] * ((1 - target) * torch.log(1 - output + eps))\n",
    "    else:\n",
    "        loss = target * torch.log(output) + (1 - target) * torch.log(1 - output + eps)\n",
    "    return torch.neg(torch.mean(loss))\n",
    "\n",
    "def customized_loss(output, target):\n",
    "    w = torch.mean(target)\n",
    "    return weighted_binary_cross_entropy(output, target, weights=[1-w, w])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 2, 3]) torch.Size([1, 1, 2, 3])\n",
      "tensor(0.6931)\n"
     ]
    }
   ],
   "source": [
    "output = torch.ones(1, 1, 2, 3) * 0.5\n",
    "target = torch.ones(1, 1, 2, 3)\n",
    "print(output.shape, target.shape)\n",
    "print(customized_loss(output, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----target-----\n",
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "torch.Size([5, 30]) torch.Size([5, 30])\n",
      "tensor(11.8479, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "target = torch.tensor(np.load('0001_f_22_02_smile_tex.npy'), dtype=torch.float64)\n",
    "\n",
    "target = target[399:404, 300:330]\n",
    "print('-----target-----')\n",
    "print(np.asarray(target, dtype='int'))\n",
    "\n",
    "output = torch.ones(target.shape)\n",
    "print(output.shape, target.shape)\n",
    "print(customized_loss(output, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

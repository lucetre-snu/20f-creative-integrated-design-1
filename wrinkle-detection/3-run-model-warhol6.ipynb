{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrinkle Detection\n",
    "#### U-net w. f-1 loss\n",
    "* https://datalab.snu.ac.kr/datalab-internal/gpu-status/\n",
    "* warhol2\n",
    "  * `jupyter lab --ip=147.46.216.82 --NotebookApp.password='sha1:6d8bb616ac21:dc1b7ebffd85cb159379a282c6b49e6121e0ffb1'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.cuda.device object at 0x7f1e3c1fbe48>\n",
      "4\n",
      "GeForce GTX 1080 Ti\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.device(0))\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.get_device_name(0))\n",
    "print(torch.cuda.is_available())\n",
    "# print(torch.cuda.memory_allocated())\n",
    "# print(torch.cuda.memoy_cached())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import datetime\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from unet.model import UNet\n",
    "from unet.dataset import *\n",
    "from unet.util import *\n",
    "from unet.train import train\n",
    "from unet.evaluate import evaluate\n",
    "from unet.loss import f1_loss, weighted_loss_and_f1_loss\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchvision import transforms, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_log(key, lr, batch_size, num_epoch, loss_name, data_dir, ckpt_dir, log_dir, mode, device, train_continue):\n",
    "    time = datetime.datetime.today()\n",
    "    time = time.strftime('%m-%d_%H-%M')\n",
    "    time_and_key_and_loss = time + \"_\" + key + '_'+ loss_name\n",
    "    if ckpt_dir == \"./checkpoint\":\n",
    "        ckpt_dir = os.path.join(ckpt_dir, time_and_key_and_loss)\n",
    "    log_dir = os.path.join(log_dir, time_and_key_and_loss)\n",
    "    if not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "        \n",
    "    print(f'tensorboard --logdir {log_dir} --host \"147.46.216.169\" --port 6006')\n",
    "    \n",
    "    # log parameters\n",
    "    print(\"learning rate: %.4e\" % lr)\n",
    "    print(\"batch size: %d\" % batch_size)\n",
    "    print(\"number of epoch: %d\" % num_epoch)\n",
    "    print(\"loss function : %s\" % loss_name)\n",
    "    print(\"data dir: %s\" % data_dir)\n",
    "    print(\"ckpt dir: %s\" % ckpt_dir)\n",
    "    print(\"log dir: %s\" % log_dir)\n",
    "    print(\"mode: %s\" % mode)\n",
    "    print(\"device: %s\" % device)\n",
    "    print(\"train_continue: %s\" % train_continue)\n",
    "    f = open(os.path.join(log_dir, 'parameter.txt'), 'w')\n",
    "    f.write(\"learning rate: %.4e\\n\" % lr)\n",
    "    f.write(\"batch size: %d\\n\" % batch_size)\n",
    "    f.write(\"number of epoch: %d\\n\" % num_epoch)\n",
    "    f.write(\"loss function : %s\\n\" % loss_name)\n",
    "    f.write(\"data dir: %s\\n\" % data_dir)\n",
    "    f.write(\"ckpt dir: %s\\n\" % ckpt_dir)\n",
    "    f.write(\"log dir: %s\\n\" % log_dir)\n",
    "    f.write(\"mode: %s\\n\" % mode)\n",
    "    f.write(\"device: %s\\n\" % device)\n",
    "    f.write(\"train_continue: %s\\n\" % train_continue)\n",
    "    f.close()\n",
    "    return ckpt_dir, log_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_unet(data_dir, loss_name='weighted_BCE', lr=1e-3, batch_size=8, num_epoch=300, ckpt_dir=\"./checkpoint\", log_dir=\"./log\", mode=\"train\", train_continue=\"off\", key='eye_left'):\n",
    "    \n",
    "    train_transform = transforms.Compose([RandomResizedCrop(ratio=0.3), Normalization(mean=0.5, std=0.5), RandomFlip(), ToTensor()])\n",
    "    val_transform = transforms.Compose([Normalization(mean=0.5, std=0.5), ToTensor()])\n",
    "\n",
    "    dataset_train = Dataset(data_dir=os.path.join(data_dir, 'train'), transform=train_transform)\n",
    "    loader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "\n",
    "    dataset_val = Dataset(data_dir=os.path.join(data_dir, 'val'), transform=val_transform)\n",
    "    loader_val = DataLoader(dataset_val, batch_size=batch_size, shuffle=False, num_workers=8)\n",
    "\n",
    "    num_data_train = len(dataset_train)\n",
    "    num_data_val = len(dataset_val)\n",
    "\n",
    "    num_batch_train = np.ceil(num_data_train / batch_size)\n",
    "    num_batch_val = np.ceil(num_data_val / batch_size)\n",
    "\n",
    "    \n",
    "    if loss_name == \"BCE\":\n",
    "        loss_function = nn.BCEWithLogitsLoss().to(device)\n",
    "    elif loss_name == \"weighted_BCE\":\n",
    "        loss_function = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([5])).to(device)  # 237 - > 59 -> 15\n",
    "    elif loss_name == \"f1\":\n",
    "        loss_function = f1_loss\n",
    "    elif loss_name ==\"mix\":\n",
    "        mix_class = weighted_loss_and_f1_loss(nn.BCEWithLogitsLoss(pos_weight=torch.tensor([15])).to(device))\n",
    "        loss_function = mix_class.loss\n",
    "    else:\n",
    "        assert False, loss_name + \" is not supported\"\n",
    "        \n",
    "    ckpt_dir, log_dir = print_log(key, lr, batch_size, num_epoch, loss_name, data_dir, ckpt_dir, log_dir, mode, device, train_continue)\n",
    "        \n",
    "    net = UNet()\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        net = nn.DataParallel(net)\n",
    "    net.to(device)\n",
    "    \n",
    "    optim = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "    writer_train = SummaryWriter(log_dir=os.path.join(log_dir, 'train'))\n",
    "    writer_val = SummaryWriter(log_dir=os.path.join(log_dir, 'val'))\n",
    "\n",
    "    train_f = open(os.path.join(log_dir, 'train.tsv'), 'w', encoding='utf-8', newline='')\n",
    "    train_wr = csv.writer(train_f, delimiter='\\t')\n",
    "    val_f = open(os.path.join(log_dir, 'val.tsv'), 'w', encoding='utf-8', newline='')\n",
    "    val_wr = csv.writer(val_f, delimiter='\\t')\n",
    "    train_wr.writerow([\"#epoch/loss/acc/f1/tp/tn/fp/fn\"])\n",
    "    val_wr.writerow([\"#epoch/loss/acc/f1/tp/tn/fp/fn\"])\n",
    "\n",
    "    ## train network\n",
    "    st_epoch = 0\n",
    "\n",
    "    if train_continue == \"on\":\n",
    "        net, optim, st_epoch = load(ckpt_dir=ckpt_dir, net=net, optim=optim)\n",
    "\n",
    "    for epoch in range(st_epoch, st_epoch + num_epoch):\n",
    "        net.train()\n",
    "        loss, acc, f1, tp, tn, fp, fn = train(net, loader_train, loss_function, num_batch_train, epoch, writer_train, device, optim)\n",
    "        train_wr.writerow((epoch, loss, acc, f1, tp, tn, fp, fn))\n",
    "\n",
    "        loss, acc, f1, tp, tn, fp, fn = evaluate(net, loader_val, loss_function, num_batch_val, epoch, writer_val, device)\n",
    "        val_wr.writerow((epoch, loss, acc, f1, tp, tn, fp, fn))\n",
    "\n",
    "        if epoch+1 == num_epoch: #((epoch+1) % (num_epoch / 10)) == 0:\n",
    "            save(ckpt_dir=ckpt_dir, net=net, optim=optim, epoch=epoch+1)\n",
    "        print(\"------------------------------------------------------------\")\n",
    "\n",
    "    writer_train.close()\n",
    "    writer_val.close()\n",
    "    train_f.close()\n",
    "    val_f.close()\n",
    "    \n",
    "    return ckpt_dir, log_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.random import randn\n",
    "\n",
    "def plot_f1(logs):\n",
    "    markers = (\"o\", \"x\", \"s\", \"^\")\n",
    "    colors = ('dodgerblue','mediumseagreen', 'hotpink', '#fba84a')\n",
    "    \n",
    "    def plot_train():\n",
    "        plt.rcParams[\"figure.figsize\"] = (16,4)\n",
    "        plt.rcParams['lines.linewidth'] = 2\n",
    "        plt.rcParams['lines.color'] = 'r'\n",
    "        plt.rcParams['axes.grid'] = True\n",
    "        plt.rcParams['axes.spines.right'] = False\n",
    "        plt.rcParams['axes.spines.top'] = False\n",
    "\n",
    "        plt.suptitle('F1 Score - Train', fontsize=20)\n",
    "\n",
    "        for i, k in enumerate(logs):\n",
    "            # epoch/loss/acc/f1/tp/tn/fp/fn\n",
    "            df = pd.read_csv(f\"{logs[k]}/val.tsv\", delimiter='\\t', header=None, skiprows=1)\n",
    "            val = df[3].to_numpy()\n",
    "            df = pd.read_csv(f\"{logs[k]}/train.tsv\", delimiter='\\t', header=None, skiprows=1)\n",
    "            train = df[3].to_numpy()\n",
    "            plt.plot(train, color=colors[i], marker=markers[i])\n",
    "        plt.legend(logs.keys(), fontsize=15)\n",
    "        plt.xlabel('epoch', fontsize=15)\n",
    "        plt.ylabel('f1 score', fontsize=15)\n",
    "        plt.show()\n",
    "    def plot_val():\n",
    "        plt.rcParams[\"figure.figsize\"] = (16,4)\n",
    "        plt.rcParams['lines.linewidth'] = 2\n",
    "        plt.rcParams['lines.color'] = 'r'\n",
    "        plt.rcParams['axes.grid'] = True\n",
    "        plt.rcParams['axes.spines.right'] = False\n",
    "        plt.rcParams['axes.spines.top'] = False\n",
    "\n",
    "        plt.suptitle('F1 Score - Validation', fontsize=20)\n",
    "\n",
    "        for i, k in enumerate(logs):\n",
    "            # epoch/loss/acc/f1/tp/tn/fp/fn\n",
    "            df = pd.read_csv(f\"{logs[k]}/val.tsv\", delimiter='\\t', header=None, skiprows=1)\n",
    "            val = df[3].to_numpy()\n",
    "            df = pd.read_csv(f\"{logs[k]}/train.tsv\", delimiter='\\t', header=None, skiprows=1)\n",
    "            train = df[3].to_numpy()\n",
    "            plt.plot(val, color=colors[i], marker=markers[i])\n",
    "        plt.legend(logs.keys(), fontsize=15)\n",
    "        plt.xlabel('epoch', fontsize=15)\n",
    "        plt.ylabel('f1 score', fontsize=15)\n",
    "        plt.show()\n",
    "    plot_train()\n",
    "    plot_val()\n",
    "    \n",
    "def plot_loss(logs):\n",
    "    markers = (\"o\", \"x\", \"s\", \"^\")\n",
    "    colors = ('dodgerblue','mediumseagreen', 'hotpink', '#fba84a')\n",
    "    \n",
    "    def plot_train():\n",
    "        plt.rcParams[\"figure.figsize\"] = (16,4)\n",
    "        plt.rcParams['lines.linewidth'] = 2\n",
    "        plt.rcParams['lines.color'] = 'r'\n",
    "        plt.rcParams['axes.grid'] = True\n",
    "        plt.rcParams['axes.spines.right'] = False\n",
    "        plt.rcParams['axes.spines.top'] = False\n",
    "\n",
    "        plt.suptitle('Loss - Train', fontsize=20)\n",
    "\n",
    "        for i, k in enumerate(logs):\n",
    "            # epoch/loss/acc/f1/tp/tn/fp/fn\n",
    "            df = pd.read_csv(f\"{logs[k]}/val.tsv\", delimiter='\\t', header=None, skiprows=1)\n",
    "            val = df[1].to_numpy()\n",
    "            df = pd.read_csv(f\"{logs[k]}/train.tsv\", delimiter='\\t', header=None, skiprows=1)\n",
    "            train = df[1].to_numpy()\n",
    "            plt.plot(train, color=colors[i], marker=markers[i])\n",
    "        plt.legend(logs.keys(), fontsize=15)\n",
    "        plt.xlabel('epoch', fontsize=15)\n",
    "        plt.ylabel('loss', fontsize=15)\n",
    "        plt.show()\n",
    "    def plot_val():\n",
    "        plt.rcParams[\"figure.figsize\"] = (16,4)\n",
    "        plt.rcParams['lines.linewidth'] = 2\n",
    "        plt.rcParams['lines.color'] = 'r'\n",
    "        plt.rcParams['axes.grid'] = True\n",
    "        plt.rcParams['axes.spines.right'] = False\n",
    "        plt.rcParams['axes.spines.top'] = False\n",
    "\n",
    "        plt.suptitle('Loss - Validation', fontsize=20)\n",
    "\n",
    "        for i, k in enumerate(logs):\n",
    "            # epoch/loss/acc/f1/tp/tn/fp/fn\n",
    "            df = pd.read_csv(f\"{logs[k]}/val.tsv\", delimiter='\\t', header=None, skiprows=1)\n",
    "            val = df[1].to_numpy()\n",
    "            df = pd.read_csv(f\"{logs[k]}/train.tsv\", delimiter='\\t', header=None, skiprows=1)\n",
    "            train = df[1].to_numpy()\n",
    "            plt.plot(val, color=colors[i], marker=markers[i])\n",
    "        plt.legend(logs.keys(), fontsize=15)\n",
    "        plt.xlabel('epoch', fontsize=15)\n",
    "        plt.ylabel('loss', fontsize=15)\n",
    "        plt.show()\n",
    "    plot_train()\n",
    "    plot_val()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### U-net baseline\n",
    "* `train_unet(data_dir, loss_name)`\n",
    "  * Loss function: `BCE`, `weight_BCE`, `f1`, `mix`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = {}\n",
    "ckpts = {}\n",
    "new_dict = {\n",
    "    'eye_left': (0, 575, 220, 603),\n",
    "    'eye_right': (672, 1247, 220, 603),\n",
    "    'nose_left': (350, 573, 400, 943),\n",
    "    'nose_right': (700, 923, 400, 943),\n",
    "    'mouth': (300, 971, 940, 1195),\n",
    "    'forehead': (0, 1247, 0, 223),\n",
    "    'cheek_right': (896, 1247, 576, 1343),\n",
    "    'cheek_left': (0, 351, 576, 1343),\n",
    "    'jaw': (350, 925, 1152, 1343),\n",
    "    'center': (550, 709, 220, 955)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorboard --logdir ./log/12-15_22-29_eye_left_BCE --host \"147.46.216.169\" --port 6006\n",
      "learning rate: 1.0000e-03\n",
      "batch size: 16\n",
      "number of epoch: 200\n",
      "loss function : BCE\n",
      "data dir: ../../../split_data/eye_left/\n",
      "ckpt dir: ./checkpoint/12-15_22-29_eye_left_BCE\n",
      "log dir: ./log/12-15_22-29_eye_left_BCE\n",
      "mode: train\n",
      "device: cuda\n",
      "train_continue: off\n",
      "TRAIN: EPOCH 0000 | BATCH 0001 / 0028 | LOSS 0.6106 | ACC 0.9363 | F1 SCORE 0.0219 | TIME 12.4751\n",
      "TRAIN: EPOCH 0000 | BATCH 0002 / 0028 | LOSS 0.5569 | ACC 0.9631 | F1 SCORE 0.0176 | TIME 12.9703\n",
      "TRAIN: EPOCH 0000 | BATCH 0003 / 0028 | LOSS 0.4816 | ACC 0.9873 | F1 SCORE 0.0156 | TIME 13.4253\n",
      "TRAIN: EPOCH 0000 | BATCH 0004 / 0028 | LOSS 0.4370 | ACC 0.9827 | F1 SCORE 0.0213 | TIME 13.8972\n",
      "TRAIN: EPOCH 0000 | BATCH 0005 / 0028 | LOSS 0.4011 | ACC 0.9802 | F1 SCORE 0.0221 | TIME 14.3922\n",
      "TRAIN: EPOCH 0000 | BATCH 0006 / 0028 | LOSS 0.3690 | ACC 0.9838 | F1 SCORE 0.0227 | TIME 15.1372\n",
      "TRAIN: EPOCH 0000 | BATCH 0007 / 0028 | LOSS 0.3622 | ACC 0.9844 | F1 SCORE 0.0210 | TIME 15.8661\n",
      "TRAIN: EPOCH 0000 | BATCH 0008 / 0028 | LOSS 0.3415 | ACC 0.9874 | F1 SCORE 0.0174 | TIME 16.3367\n",
      "TRAIN: EPOCH 0000 | BATCH 0009 / 0028 | LOSS 0.3316 | ACC 0.9837 | F1 SCORE 0.0223 | TIME 16.8179\n",
      "TRAIN: EPOCH 0000 | BATCH 0010 / 0028 | LOSS 0.3222 | ACC 0.9887 | F1 SCORE 0.0152 | TIME 17.2801\n",
      "TRAIN: EPOCH 0000 | BATCH 0011 / 0028 | LOSS 0.3066 | ACC 0.9811 | F1 SCORE 0.0254 | TIME 17.7712\n",
      "TRAIN: EPOCH 0000 | BATCH 0012 / 0028 | LOSS 0.3031 | ACC 0.9857 | F1 SCORE 0.0184 | TIME 18.2542\n",
      "TRAIN: EPOCH 0000 | BATCH 0013 / 0028 | LOSS 0.2853 | ACC 0.9836 | F1 SCORE 0.0224 | TIME 18.7091\n",
      "TRAIN: EPOCH 0000 | BATCH 0014 / 0028 | LOSS 0.2883 | ACC 0.9849 | F1 SCORE 0.0199 | TIME 19.1669\n",
      "TRAIN: EPOCH 0000 | BATCH 0015 / 0028 | LOSS 0.2674 | ACC 0.9880 | F1 SCORE 0.0160 | TIME 19.6203\n",
      "TRAIN: EPOCH 0000 | BATCH 0016 / 0028 | LOSS 0.2924 | ACC 0.9826 | F1 SCORE 0.0241 | TIME 20.0759\n",
      "TRAIN: EPOCH 0000 | BATCH 0017 / 0028 | LOSS 0.2607 | ACC 0.9854 | F1 SCORE 0.0200 | TIME 20.5367\n",
      "TRAIN: EPOCH 0000 | BATCH 0018 / 0028 | LOSS 0.2511 | ACC 0.9873 | F1 SCORE 0.0187 | TIME 21.0226\n",
      "TRAIN: EPOCH 0000 | BATCH 0019 / 0028 | LOSS 0.2516 | ACC 0.9859 | F1 SCORE 0.0198 | TIME 21.4807\n",
      "TRAIN: EPOCH 0000 | BATCH 0020 / 0028 | LOSS 0.2426 | ACC 0.9872 | F1 SCORE 0.0173 | TIME 21.9852\n",
      "TRAIN: EPOCH 0000 | BATCH 0021 / 0028 | LOSS 0.2367 | ACC 0.9862 | F1 SCORE 0.0188 | TIME 22.5007\n",
      "TRAIN: EPOCH 0000 | BATCH 0022 / 0028 | LOSS 0.2249 | ACC 0.9858 | F1 SCORE 0.0188 | TIME 23.0026\n",
      "TRAIN: EPOCH 0000 | BATCH 0023 / 0028 | LOSS 0.2263 | ACC 0.9852 | F1 SCORE 0.0195 | TIME 23.4827\n",
      "TRAIN: EPOCH 0000 | BATCH 0024 / 0028 | LOSS 0.2170 | ACC 0.9871 | F1 SCORE 0.0175 | TIME 23.9432\n",
      "TRAIN: EPOCH 0000 | BATCH 0025 / 0028 | LOSS 0.2271 | ACC 0.9791 | F1 SCORE 0.0282 | TIME 24.4032\n",
      "TRAIN: EPOCH 0000 | BATCH 0026 / 0028 | LOSS 0.2143 | ACC 0.9861 | F1 SCORE 0.0184 | TIME 24.8650\n",
      "TRAIN: EPOCH 0000 | BATCH 0027 / 0028 | LOSS 0.2017 | ACC 0.9830 | F1 SCORE 0.0238 | TIME 25.3225\n",
      "TRAIN: EPOCH 0000 | BATCH 0028 / 0028 | LOSS 0.2187 | ACC 0.9764 | F1 SCORE 0.0305 | TIME 25.4715\n",
      "VALID: EPOCH 0000 | BATCH 0001 / 0002 | LOSS 0.2159 | ACC 0.9952 | F1 SCORE 0.0090\n",
      "VALID: EPOCH 0000 | BATCH 0002 / 0002 | LOSS 0.2136 | ACC 0.9963 | F1 SCORE 0.0069\n",
      "------------------------------------------------------------\n",
      "TRAIN: EPOCH 0001 | BATCH 0001 / 0028 | LOSS 0.1933 | ACC 0.9872 | F1 SCORE 0.0170 | TIME 1.9157\n",
      "TRAIN: EPOCH 0001 | BATCH 0002 / 0028 | LOSS 0.1993 | ACC 0.9815 | F1 SCORE 0.0259 | TIME 2.4027\n",
      "TRAIN: EPOCH 0001 | BATCH 0003 / 0028 | LOSS 0.1910 | ACC 0.9857 | F1 SCORE 0.0198 | TIME 2.8620\n",
      "TRAIN: EPOCH 0001 | BATCH 0004 / 0028 | LOSS 0.1882 | ACC 0.9845 | F1 SCORE 0.0207 | TIME 3.3431\n",
      "TRAIN: EPOCH 0001 | BATCH 0005 / 0028 | LOSS 0.1871 | ACC 0.9866 | F1 SCORE 0.0187 | TIME 3.8035\n",
      "TRAIN: EPOCH 0001 | BATCH 0006 / 0028 | LOSS 0.1804 | ACC 0.9847 | F1 SCORE 0.0203 | TIME 4.2645\n",
      "TRAIN: EPOCH 0001 | BATCH 0007 / 0028 | LOSS 0.1749 | ACC 0.9857 | F1 SCORE 0.0195 | TIME 4.7274\n",
      "TRAIN: EPOCH 0001 | BATCH 0008 / 0028 | LOSS 0.1720 | ACC 0.9850 | F1 SCORE 0.0207 | TIME 5.1888\n",
      "TRAIN: EPOCH 0001 | BATCH 0009 / 0028 | LOSS 0.1658 | ACC 0.9849 | F1 SCORE 0.0195 | TIME 5.6525\n",
      "TRAIN: EPOCH 0001 | BATCH 0010 / 0028 | LOSS 0.1608 | ACC 0.9879 | F1 SCORE 0.0170 | TIME 6.1291\n",
      "TRAIN: EPOCH 0001 | BATCH 0011 / 0028 | LOSS 0.1606 | ACC 0.9845 | F1 SCORE 0.0209 | TIME 6.6042\n",
      "TRAIN: EPOCH 0001 | BATCH 0012 / 0028 | LOSS 0.1605 | ACC 0.9813 | F1 SCORE 0.0261 | TIME 7.0616\n",
      "TRAIN: EPOCH 0001 | BATCH 0013 / 0028 | LOSS 0.1532 | ACC 0.9850 | F1 SCORE 0.0206 | TIME 7.5218\n",
      "TRAIN: EPOCH 0001 | BATCH 0014 / 0028 | LOSS 0.1568 | ACC 0.9806 | F1 SCORE 0.0263 | TIME 7.9846\n",
      "TRAIN: EPOCH 0001 | BATCH 0015 / 0028 | LOSS 0.1513 | ACC 0.9818 | F1 SCORE 0.0245 | TIME 8.4704\n",
      "TRAIN: EPOCH 0001 | BATCH 0016 / 0028 | LOSS 0.1449 | ACC 0.9840 | F1 SCORE 0.0219 | TIME 8.9333\n",
      "TRAIN: EPOCH 0001 | BATCH 0017 / 0028 | LOSS 0.1426 | ACC 0.9850 | F1 SCORE 0.0214 | TIME 9.4157\n",
      "TRAIN: EPOCH 0001 | BATCH 0018 / 0028 | LOSS 0.1499 | ACC 0.9863 | F1 SCORE 0.0189 | TIME 9.8770\n",
      "TRAIN: EPOCH 0001 | BATCH 0019 / 0028 | LOSS 0.1356 | ACC 0.9867 | F1 SCORE 0.0186 | TIME 10.3371\n",
      "TRAIN: EPOCH 0001 | BATCH 0020 / 0028 | LOSS 0.1354 | ACC 0.9868 | F1 SCORE 0.0192 | TIME 10.8073\n",
      "TRAIN: EPOCH 0001 | BATCH 0021 / 0028 | LOSS 0.1335 | ACC 0.9875 | F1 SCORE 0.0167 | TIME 11.2850\n",
      "TRAIN: EPOCH 0001 | BATCH 0022 / 0028 | LOSS 0.1279 | ACC 0.9893 | F1 SCORE 0.0145 | TIME 11.7475\n",
      "TRAIN: EPOCH 0001 | BATCH 0023 / 0028 | LOSS 0.1301 | ACC 0.9855 | F1 SCORE 0.0202 | TIME 12.2089\n",
      "TRAIN: EPOCH 0001 | BATCH 0024 / 0028 | LOSS 0.1326 | ACC 0.9847 | F1 SCORE 0.0228 | TIME 12.6704\n",
      "TRAIN: EPOCH 0001 | BATCH 0025 / 0028 | LOSS 0.1250 | ACC 0.9869 | F1 SCORE 0.0182 | TIME 13.1356\n",
      "TRAIN: EPOCH 0001 | BATCH 0026 / 0028 | LOSS 0.1236 | ACC 0.9865 | F1 SCORE 0.0190 | TIME 13.6140\n",
      "TRAIN: EPOCH 0001 | BATCH 0027 / 0028 | LOSS 0.1288 | ACC 0.9828 | F1 SCORE 0.0248 | TIME 14.0991\n",
      "TRAIN: EPOCH 0001 | BATCH 0028 / 0028 | LOSS 0.1306 | ACC 0.9856 | F1 SCORE 0.0185 | TIME 14.2563\n",
      "VALID: EPOCH 0001 | BATCH 0001 / 0002 | LOSS 0.1364 | ACC 0.9952 | F1 SCORE 0.0093\n",
      "VALID: EPOCH 0001 | BATCH 0002 / 0002 | LOSS 0.1335 | ACC 0.9963 | F1 SCORE 0.0071\n",
      "------------------------------------------------------------\n",
      "TRAIN: EPOCH 0002 | BATCH 0001 / 0028 | LOSS 0.1209 | ACC 0.9849 | F1 SCORE 0.0222 | TIME 2.0004\n",
      "TRAIN: EPOCH 0002 | BATCH 0002 / 0028 | LOSS 0.1201 | ACC 0.9845 | F1 SCORE 0.0227 | TIME 2.5253\n",
      "TRAIN: EPOCH 0002 | BATCH 0003 / 0028 | LOSS 0.1200 | ACC 0.9839 | F1 SCORE 0.0225 | TIME 2.9888\n",
      "TRAIN: EPOCH 0002 | BATCH 0004 / 0028 | LOSS 0.1115 | ACC 0.9884 | F1 SCORE 0.0164 | TIME 3.4521\n",
      "TRAIN: EPOCH 0002 | BATCH 0005 / 0028 | LOSS 0.1207 | ACC 0.9814 | F1 SCORE 0.0272 | TIME 3.9149\n",
      "TRAIN: EPOCH 0002 | BATCH 0006 / 0028 | LOSS 0.1186 | ACC 0.9813 | F1 SCORE 0.0271 | TIME 4.3901\n",
      "TRAIN: EPOCH 0002 | BATCH 0007 / 0028 | LOSS 0.1101 | ACC 0.9854 | F1 SCORE 0.0226 | TIME 4.8576\n",
      "TRAIN: EPOCH 0002 | BATCH 0008 / 0028 | LOSS 0.1096 | ACC 0.9871 | F1 SCORE 0.0189 | TIME 5.3247\n",
      "TRAIN: EPOCH 0002 | BATCH 0009 / 0028 | LOSS 0.1036 | ACC 0.9883 | F1 SCORE 0.0163 | TIME 5.8142\n",
      "TRAIN: EPOCH 0002 | BATCH 0010 / 0028 | LOSS 0.1088 | ACC 0.9841 | F1 SCORE 0.0221 | TIME 6.2854\n",
      "TRAIN: EPOCH 0002 | BATCH 0011 / 0028 | LOSS 0.1042 | ACC 0.9865 | F1 SCORE 0.0208 | TIME 6.7666\n",
      "TRAIN: EPOCH 0002 | BATCH 0012 / 0028 | LOSS 0.1063 | ACC 0.9848 | F1 SCORE 0.0217 | TIME 7.3135\n",
      "TRAIN: EPOCH 0002 | BATCH 0013 / 0028 | LOSS 0.1052 | ACC 0.9844 | F1 SCORE 0.0231 | TIME 7.9642\n",
      "TRAIN: EPOCH 0002 | BATCH 0014 / 0028 | LOSS 0.0958 | ACC 0.9897 | F1 SCORE 0.0153 | TIME 8.4444\n",
      "TRAIN: EPOCH 0002 | BATCH 0015 / 0028 | LOSS 0.1067 | ACC 0.9826 | F1 SCORE 0.0260 | TIME 8.9322\n",
      "TRAIN: EPOCH 0002 | BATCH 0016 / 0028 | LOSS 0.1002 | ACC 0.9850 | F1 SCORE 0.0233 | TIME 9.4133\n",
      "TRAIN: EPOCH 0002 | BATCH 0017 / 0028 | LOSS 0.0950 | ACC 0.9882 | F1 SCORE 0.0190 | TIME 9.8750\n",
      "TRAIN: EPOCH 0002 | BATCH 0018 / 0028 | LOSS 0.0921 | ACC 0.9886 | F1 SCORE 0.0168 | TIME 10.3358\n",
      "TRAIN: EPOCH 0002 | BATCH 0019 / 0028 | LOSS 0.0968 | ACC 0.9855 | F1 SCORE 0.0210 | TIME 10.7973\n",
      "TRAIN: EPOCH 0002 | BATCH 0020 / 0028 | LOSS 0.0973 | ACC 0.9838 | F1 SCORE 0.0252 | TIME 11.2755\n",
      "TRAIN: EPOCH 0002 | BATCH 0021 / 0028 | LOSS 0.0972 | ACC 0.9834 | F1 SCORE 0.0254 | TIME 11.9001\n",
      "TRAIN: EPOCH 0002 | BATCH 0022 / 0028 | LOSS 0.0983 | ACC 0.9823 | F1 SCORE 0.0274 | TIME 12.6336\n",
      "TRAIN: EPOCH 0002 | BATCH 0023 / 0028 | LOSS 0.0968 | ACC 0.9836 | F1 SCORE 0.0238 | TIME 13.1013\n",
      "TRAIN: EPOCH 0002 | BATCH 0024 / 0028 | LOSS 0.1007 | ACC 0.9803 | F1 SCORE 0.0316 | TIME 13.5861\n",
      "TRAIN: EPOCH 0002 | BATCH 0025 / 0028 | LOSS 0.0911 | ACC 0.9853 | F1 SCORE 0.0228 | TIME 14.0530\n",
      "TRAIN: EPOCH 0002 | BATCH 0026 / 0028 | LOSS 0.0868 | ACC 0.9877 | F1 SCORE 0.0202 | TIME 14.5366\n",
      "TRAIN: EPOCH 0002 | BATCH 0027 / 0028 | LOSS 0.0892 | ACC 0.9855 | F1 SCORE 0.0228 | TIME 14.9983\n",
      "TRAIN: EPOCH 0002 | BATCH 0028 / 0028 | LOSS 0.0888 | ACC 0.9897 | F1 SCORE 0.0159 | TIME 15.1721\n",
      "VALID: EPOCH 0002 | BATCH 0001 / 0002 | LOSS 0.0763 | ACC 0.9952 | F1 SCORE 0.0100\n",
      "VALID: EPOCH 0002 | BATCH 0002 / 0002 | LOSS 0.0736 | ACC 0.9963 | F1 SCORE 0.0077\n",
      "------------------------------------------------------------\n",
      "TRAIN: EPOCH 0003 | BATCH 0001 / 0028 | LOSS 0.0961 | ACC 0.9798 | F1 SCORE 0.0332 | TIME 1.9454\n",
      "TRAIN: EPOCH 0003 | BATCH 0002 / 0028 | LOSS 0.0902 | ACC 0.9845 | F1 SCORE 0.0231 | TIME 2.4677\n",
      "TRAIN: EPOCH 0003 | BATCH 0003 / 0028 | LOSS 0.0950 | ACC 0.9818 | F1 SCORE 0.0298 | TIME 2.9317\n",
      "TRAIN: EPOCH 0003 | BATCH 0004 / 0028 | LOSS 0.0845 | ACC 0.9867 | F1 SCORE 0.0228 | TIME 3.3940\n",
      "TRAIN: EPOCH 0003 | BATCH 0005 / 0028 | LOSS 0.0882 | ACC 0.9837 | F1 SCORE 0.0278 | TIME 3.8642\n",
      "TRAIN: EPOCH 0003 | BATCH 0006 / 0028 | LOSS 0.0817 | ACC 0.9874 | F1 SCORE 0.0217 | TIME 4.3242\n",
      "TRAIN: EPOCH 0003 | BATCH 0007 / 0028 | LOSS 0.0850 | ACC 0.9850 | F1 SCORE 0.0254 | TIME 4.7905\n",
      "TRAIN: EPOCH 0003 | BATCH 0008 / 0028 | LOSS 0.0804 | ACC 0.9869 | F1 SCORE 0.0210 | TIME 5.2524\n",
      "TRAIN: EPOCH 0003 | BATCH 0009 / 0028 | LOSS 0.0800 | ACC 0.9872 | F1 SCORE 0.0205 | TIME 5.7153\n",
      "TRAIN: EPOCH 0003 | BATCH 0010 / 0028 | LOSS 0.0788 | ACC 0.9870 | F1 SCORE 0.0220 | TIME 6.2144\n",
      "TRAIN: EPOCH 0003 | BATCH 0011 / 0028 | LOSS 0.0811 | ACC 0.9857 | F1 SCORE 0.0242 | TIME 6.6845\n",
      "TRAIN: EPOCH 0003 | BATCH 0012 / 0028 | LOSS 0.0839 | ACC 0.9851 | F1 SCORE 0.0243 | TIME 7.2489\n",
      "TRAIN: EPOCH 0003 | BATCH 0013 / 0028 | LOSS 0.0762 | ACC 0.9874 | F1 SCORE 0.0262 | TIME 7.9675\n",
      "TRAIN: EPOCH 0003 | BATCH 0014 / 0028 | LOSS 0.0807 | ACC 0.9853 | F1 SCORE 0.0243 | TIME 8.4422\n",
      "TRAIN: EPOCH 0003 | BATCH 0015 / 0028 | LOSS 0.0789 | ACC 0.9856 | F1 SCORE 0.0256 | TIME 8.9147\n",
      "TRAIN: EPOCH 0003 | BATCH 0016 / 0028 | LOSS 0.0835 | ACC 0.9831 | F1 SCORE 0.0264 | TIME 9.3939\n",
      "TRAIN: EPOCH 0003 | BATCH 0017 / 0028 | LOSS 0.0755 | ACC 0.9863 | F1 SCORE 0.0270 | TIME 9.8774\n",
      "TRAIN: EPOCH 0003 | BATCH 0018 / 0028 | LOSS 0.0796 | ACC 0.9851 | F1 SCORE 0.0272 | TIME 10.4257\n",
      "TRAIN: EPOCH 0003 | BATCH 0019 / 0028 | LOSS 0.0810 | ACC 0.9840 | F1 SCORE 0.0269 | TIME 10.8863\n",
      "TRAIN: EPOCH 0003 | BATCH 0020 / 0028 | LOSS 0.0747 | ACC 0.9855 | F1 SCORE 0.0305 | TIME 11.3476\n",
      "TRAIN: EPOCH 0003 | BATCH 0021 / 0028 | LOSS 0.0798 | ACC 0.9826 | F1 SCORE 0.0343 | TIME 11.8194\n",
      "TRAIN: EPOCH 0003 | BATCH 0022 / 0028 | LOSS 0.0700 | ACC 0.9882 | F1 SCORE 0.0269 | TIME 12.2829\n",
      "TRAIN: EPOCH 0003 | BATCH 0023 / 0028 | LOSS 0.0745 | ACC 0.9849 | F1 SCORE 0.0390 | TIME 12.7514\n",
      "TRAIN: EPOCH 0003 | BATCH 0024 / 0028 | LOSS 0.0764 | ACC 0.9833 | F1 SCORE 0.0412 | TIME 13.2185\n",
      "TRAIN: EPOCH 0003 | BATCH 0025 / 0028 | LOSS 0.0733 | ACC 0.9854 | F1 SCORE 0.0298 | TIME 13.6845\n",
      "TRAIN: EPOCH 0003 | BATCH 0026 / 0028 | LOSS 0.0751 | ACC 0.9848 | F1 SCORE 0.0335 | TIME 14.1501\n",
      "TRAIN: EPOCH 0003 | BATCH 0027 / 0028 | LOSS 0.0732 | ACC 0.9845 | F1 SCORE 0.0370 | TIME 14.6148\n",
      "TRAIN: EPOCH 0003 | BATCH 0028 / 0028 | LOSS 0.0646 | ACC 0.9913 | F1 SCORE 0.0204 | TIME 14.7679\n",
      "VALID: EPOCH 0003 | BATCH 0001 / 0002 | LOSS 0.0609 | ACC 0.9952 | F1 SCORE 0.0110\n",
      "VALID: EPOCH 0003 | BATCH 0002 / 0002 | LOSS 0.0576 | ACC 0.9963 | F1 SCORE 0.0092\n",
      "------------------------------------------------------------\n",
      "TRAIN: EPOCH 0004 | BATCH 0001 / 0028 | LOSS 0.0720 | ACC 0.9856 | F1 SCORE 0.0284 | TIME 1.9368\n",
      "TRAIN: EPOCH 0004 | BATCH 0002 / 0028 | LOSS 0.0746 | ACC 0.9841 | F1 SCORE 0.0302 | TIME 2.4305\n",
      "TRAIN: EPOCH 0004 | BATCH 0003 / 0028 | LOSS 0.0661 | ACC 0.9878 | F1 SCORE 0.0299 | TIME 2.8978\n",
      "TRAIN: EPOCH 0004 | BATCH 0004 / 0028 | LOSS 0.0736 | ACC 0.9844 | F1 SCORE 0.0386 | TIME 3.3602\n",
      "TRAIN: EPOCH 0004 | BATCH 0005 / 0028 | LOSS 0.0750 | ACC 0.9830 | F1 SCORE 0.0338 | TIME 3.8290\n",
      "TRAIN: EPOCH 0004 | BATCH 0006 / 0028 | LOSS 0.0701 | ACC 0.9853 | F1 SCORE 0.0288 | TIME 4.3059\n",
      "TRAIN: EPOCH 0004 | BATCH 0007 / 0028 | LOSS 0.0665 | ACC 0.9875 | F1 SCORE 0.0244 | TIME 4.7788\n",
      "TRAIN: EPOCH 0004 | BATCH 0008 / 0028 | LOSS 0.0677 | ACC 0.9854 | F1 SCORE 0.0402 | TIME 5.2623\n",
      "TRAIN: EPOCH 0004 | BATCH 0009 / 0028 | LOSS 0.0663 | ACC 0.9867 | F1 SCORE 0.0328 | TIME 5.7355\n",
      "TRAIN: EPOCH 0004 | BATCH 0010 / 0028 | LOSS 0.0743 | ACC 0.9823 | F1 SCORE 0.0533 | TIME 6.2019\n",
      "TRAIN: EPOCH 0004 | BATCH 0011 / 0028 | LOSS 0.0695 | ACC 0.9848 | F1 SCORE 0.0461 | TIME 6.6709\n",
      "TRAIN: EPOCH 0004 | BATCH 0012 / 0028 | LOSS 0.0662 | ACC 0.9859 | F1 SCORE 0.0318 | TIME 7.1403\n",
      "TRAIN: EPOCH 0004 | BATCH 0013 / 0028 | LOSS 0.0709 | ACC 0.9833 | F1 SCORE 0.0398 | TIME 7.6141\n",
      "TRAIN: EPOCH 0004 | BATCH 0014 / 0028 | LOSS 0.0681 | ACC 0.9845 | F1 SCORE 0.0362 | TIME 8.0769\n",
      "TRAIN: EPOCH 0004 | BATCH 0015 / 0028 | LOSS 0.0697 | ACC 0.9840 | F1 SCORE 0.0384 | TIME 8.5384\n",
      "TRAIN: EPOCH 0004 | BATCH 0016 / 0028 | LOSS 0.0657 | ACC 0.9854 | F1 SCORE 0.0453 | TIME 9.0097\n",
      "TRAIN: EPOCH 0004 | BATCH 0017 / 0028 | LOSS 0.0673 | ACC 0.9863 | F1 SCORE 0.0294 | TIME 9.4909\n",
      "TRAIN: EPOCH 0004 | BATCH 0018 / 0028 | LOSS 0.0698 | ACC 0.9829 | F1 SCORE 0.0419 | TIME 9.9533\n",
      "TRAIN: EPOCH 0004 | BATCH 0019 / 0028 | LOSS 0.0703 | ACC 0.9831 | F1 SCORE 0.0407 | TIME 10.4239\n",
      "TRAIN: EPOCH 0004 | BATCH 0020 / 0028 | LOSS 0.0589 | ACC 0.9898 | F1 SCORE 0.0189 | TIME 10.8958\n",
      "TRAIN: EPOCH 0004 | BATCH 0021 / 0028 | LOSS 0.0721 | ACC 0.9820 | F1 SCORE 0.0404 | TIME 11.3680\n",
      "TRAIN: EPOCH 0004 | BATCH 0022 / 0028 | LOSS 0.0635 | ACC 0.9864 | F1 SCORE 0.0316 | TIME 11.8329\n",
      "TRAIN: EPOCH 0004 | BATCH 0023 / 0028 | LOSS 0.0631 | ACC 0.9864 | F1 SCORE 0.0304 | TIME 12.3012\n",
      "TRAIN: EPOCH 0004 | BATCH 0024 / 0028 | LOSS 0.0561 | ACC 0.9890 | F1 SCORE 0.0340 | TIME 12.7668\n",
      "TRAIN: EPOCH 0004 | BATCH 0025 / 0028 | LOSS 0.0690 | ACC 0.9822 | F1 SCORE 0.0535 | TIME 13.3115\n",
      "TRAIN: EPOCH 0004 | BATCH 0026 / 0028 | LOSS 0.0628 | ACC 0.9857 | F1 SCORE 0.0440 | TIME 14.0051\n",
      "TRAIN: EPOCH 0004 | BATCH 0027 / 0028 | LOSS 0.0626 | ACC 0.9862 | F1 SCORE 0.0418 | TIME 14.4849\n",
      "TRAIN: EPOCH 0004 | BATCH 0028 / 0028 | LOSS 0.0641 | ACC 0.9835 | F1 SCORE 0.0428 | TIME 14.6370\n",
      "VALID: EPOCH 0004 | BATCH 0001 / 0002 | LOSS 0.0668 | ACC 0.9952 | F1 SCORE 0.0151\n",
      "VALID: EPOCH 0004 | BATCH 0002 / 0002 | LOSS 0.0650 | ACC 0.9963 | F1 SCORE 0.0122\n",
      "------------------------------------------------------------\n",
      "TRAIN: EPOCH 0005 | BATCH 0001 / 0028 | LOSS 0.0620 | ACC 0.9862 | F1 SCORE 0.0334 | TIME 1.9243\n",
      "TRAIN: EPOCH 0005 | BATCH 0002 / 0028 | LOSS 0.0704 | ACC 0.9818 | F1 SCORE 0.0409 | TIME 2.4190\n",
      "TRAIN: EPOCH 0005 | BATCH 0003 / 0028 | LOSS 0.0602 | ACC 0.9866 | F1 SCORE 0.0318 | TIME 2.8824\n",
      "TRAIN: EPOCH 0005 | BATCH 0004 / 0028 | LOSS 0.0615 | ACC 0.9855 | F1 SCORE 0.0434 | TIME 3.3645\n",
      "TRAIN: EPOCH 0005 | BATCH 0005 / 0028 | LOSS 0.0551 | ACC 0.9885 | F1 SCORE 0.0444 | TIME 3.8320\n",
      "TRAIN: EPOCH 0005 | BATCH 0006 / 0028 | LOSS 0.0650 | ACC 0.9839 | F1 SCORE 0.0458 | TIME 4.3020\n",
      "TRAIN: EPOCH 0005 | BATCH 0007 / 0028 | LOSS 0.0532 | ACC 0.9890 | F1 SCORE 0.0343 | TIME 4.7932\n",
      "TRAIN: EPOCH 0005 | BATCH 0008 / 0028 | LOSS 0.0664 | ACC 0.9829 | F1 SCORE 0.0426 | TIME 5.2582\n",
      "TRAIN: EPOCH 0005 | BATCH 0009 / 0028 | LOSS 0.0655 | ACC 0.9825 | F1 SCORE 0.0530 | TIME 5.7275\n",
      "TRAIN: EPOCH 0005 | BATCH 0010 / 0028 | LOSS 0.0554 | ACC 0.9877 | F1 SCORE 0.0404 | TIME 6.2005\n",
      "TRAIN: EPOCH 0005 | BATCH 0011 / 0028 | LOSS 0.0571 | ACC 0.9868 | F1 SCORE 0.0484 | TIME 6.6705\n",
      "TRAIN: EPOCH 0005 | BATCH 0012 / 0028 | LOSS 0.0602 | ACC 0.9865 | F1 SCORE 0.0339 | TIME 7.1343\n",
      "TRAIN: EPOCH 0005 | BATCH 0013 / 0028 | LOSS 0.0550 | ACC 0.9889 | F1 SCORE 0.0192 | TIME 7.5977\n",
      "TRAIN: EPOCH 0005 | BATCH 0014 / 0028 | LOSS 0.0624 | ACC 0.9843 | F1 SCORE 0.0394 | TIME 8.0591\n",
      "TRAIN: EPOCH 0005 | BATCH 0015 / 0028 | LOSS 0.0621 | ACC 0.9847 | F1 SCORE 0.0366 | TIME 8.5227\n",
      "TRAIN: EPOCH 0005 | BATCH 0016 / 0028 | LOSS 0.0564 | ACC 0.9870 | F1 SCORE 0.0341 | TIME 8.9855\n",
      "TRAIN: EPOCH 0005 | BATCH 0017 / 0028 | LOSS 0.0640 | ACC 0.9834 | F1 SCORE 0.0504 | TIME 9.4727\n",
      "TRAIN: EPOCH 0005 | BATCH 0018 / 0028 | LOSS 0.0651 | ACC 0.9820 | F1 SCORE 0.0632 | TIME 9.9584\n",
      "TRAIN: EPOCH 0005 | BATCH 0019 / 0028 | LOSS 0.0628 | ACC 0.9825 | F1 SCORE 0.0605 | TIME 10.4246\n",
      "TRAIN: EPOCH 0005 | BATCH 0020 / 0028 | LOSS 0.0574 | ACC 0.9861 | F1 SCORE 0.0388 | TIME 10.9104\n",
      "TRAIN: EPOCH 0005 | BATCH 0021 / 0028 | LOSS 0.0632 | ACC 0.9817 | F1 SCORE 0.0811 | TIME 11.3761\n",
      "TRAIN: EPOCH 0005 | BATCH 0022 / 0028 | LOSS 0.0563 | ACC 0.9856 | F1 SCORE 0.0596 | TIME 11.8436\n",
      "TRAIN: EPOCH 0005 | BATCH 0023 / 0028 | LOSS 0.0610 | ACC 0.9841 | F1 SCORE 0.0545 | TIME 12.3130\n",
      "TRAIN: EPOCH 0005 | BATCH 0024 / 0028 | LOSS 0.0589 | ACC 0.9845 | F1 SCORE 0.0530 | TIME 12.7813\n",
      "TRAIN: EPOCH 0005 | BATCH 0025 / 0028 | LOSS 0.0578 | ACC 0.9851 | F1 SCORE 0.0533 | TIME 13.2667\n",
      "TRAIN: EPOCH 0005 | BATCH 0026 / 0028 | LOSS 0.0583 | ACC 0.9848 | F1 SCORE 0.0509 | TIME 13.7406\n",
      "TRAIN: EPOCH 0005 | BATCH 0027 / 0028 | LOSS 0.0556 | ACC 0.9870 | F1 SCORE 0.0288 | TIME 14.2065\n",
      "TRAIN: EPOCH 0005 | BATCH 0028 / 0028 | LOSS 0.0661 | ACC 0.9817 | F1 SCORE 0.0345 | TIME 14.3638\n",
      "VALID: EPOCH 0005 | BATCH 0001 / 0002 | LOSS 0.0520 | ACC 0.9952 | F1 SCORE 0.0128\n",
      "VALID: EPOCH 0005 | BATCH 0002 / 0002 | LOSS 0.0486 | ACC 0.9963 | F1 SCORE 0.0106\n",
      "------------------------------------------------------------\n",
      "TRAIN: EPOCH 0006 | BATCH 0001 / 0028 | LOSS 0.0544 | ACC 0.9866 | F1 SCORE 0.0506 | TIME 1.9560\n",
      "TRAIN: EPOCH 0006 | BATCH 0002 / 0028 | LOSS 0.0512 | ACC 0.9882 | F1 SCORE 0.0567 | TIME 2.4666\n",
      "TRAIN: EPOCH 0006 | BATCH 0003 / 0028 | LOSS 0.0535 | ACC 0.9867 | F1 SCORE 0.0425 | TIME 2.9326\n",
      "TRAIN: EPOCH 0006 | BATCH 0004 / 0028 | LOSS 0.0548 | ACC 0.9864 | F1 SCORE 0.0351 | TIME 3.4159\n",
      "TRAIN: EPOCH 0006 | BATCH 0005 / 0028 | LOSS 0.0635 | ACC 0.9814 | F1 SCORE 0.0502 | TIME 3.8860\n",
      "TRAIN: EPOCH 0006 | BATCH 0006 / 0028 | LOSS 0.0550 | ACC 0.9865 | F1 SCORE 0.0393 | TIME 4.3606\n",
      "TRAIN: EPOCH 0006 | BATCH 0007 / 0028 | LOSS 0.0508 | ACC 0.9870 | F1 SCORE 0.0547 | TIME 4.8609\n",
      "TRAIN: EPOCH 0006 | BATCH 0008 / 0028 | LOSS 0.0597 | ACC 0.9841 | F1 SCORE 0.0512 | TIME 5.3370\n",
      "TRAIN: EPOCH 0006 | BATCH 0009 / 0028 | LOSS 0.0515 | ACC 0.9884 | F1 SCORE 0.0324 | TIME 5.8236\n",
      "TRAIN: EPOCH 0006 | BATCH 0010 / 0028 | LOSS 0.0540 | ACC 0.9860 | F1 SCORE 0.0541 | TIME 6.2881\n",
      "TRAIN: EPOCH 0006 | BATCH 0011 / 0028 | LOSS 0.0577 | ACC 0.9844 | F1 SCORE 0.0435 | TIME 6.7554\n",
      "TRAIN: EPOCH 0006 | BATCH 0012 / 0028 | LOSS 0.0551 | ACC 0.9854 | F1 SCORE 0.0461 | TIME 7.2241\n"
     ]
    }
   ],
   "source": [
    "for key in new_dict:\n",
    "    new_logs = {}\n",
    "    new_ckpts = {}\n",
    "    for loss in ('BCE', 'weighted_BCE', 'f1', 'mix'):\n",
    "        new_ckpts[loss], new_logs[loss] = train_unet(f\"../../../split_data/{key}/\", loss, num_epoch=200, batch_size=16, key=key)\n",
    "    logs[key] = new_logs\n",
    "    ckpts[key] = new_ckpts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eye_logs['BCE'] = \"./log/11-30_17-26_BCE\"\n",
    "eye_logs['weighted_BCE'] = \"./log/11-30_18-34_weighted_BCE\"\n",
    "eye_logs['f1'] = \"./log/11-30_19-41_f1\"\n",
    "eye_logs['mix'] = \"./log/11-30_20-49_mix\"\n",
    "\n",
    "plot_f1(eye_logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(eye_logs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mykernel1",
   "language": "python",
   "name": "mykernel1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
